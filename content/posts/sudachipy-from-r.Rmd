---
title: RからSudachiPyを呼ぶ
author: paithiov909
date: '2020-07-17'
lastmod: "`r Sys.Date()`"
slug: sudachipy-from-r
categories: []
tags:
  - NLP
description: 'RからSudachiPyを呼べるよ'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)

stopifnot(require(tidyverse))
stopifnot(require(reticulate))
```

## SudachiPyのロード

sapCyの日本語のモデルを入れたらなんか入った気がしたのでじゃあ呼べるのではと思っただけ。

[WorksApplications/SudachiPy: Python version of Sudachi, a Japanese tokenizer.](https://github.com/WorksApplications/SudachiPy)

```{r}
library(reticulate)
reticulate::use_condaenv(condaenv = "spacy_condaenv", required = TRUE)

tokenizer <- reticulate::import("sudachipy.tokenizer")
dictionary <- reticulate::import("sudachipy.dictionary")

tokenizer_obj <- dictionary$Dictionary()$create()
```

## Multi-granular Tokenization

分かち書きのレベルを指定できるんですよね、というあれ。Pythonだと次のような書き方をするのだけれど

```py
mode = tokenizer.Tokenizer.SplitMode.C
[m.surface() for m in tokenizer_obj.tokenize("国家公務員", mode)]
# => ['国家公務員']
```

{reticulate}でキャストされる`tokenizer_obj$tokenize()`の戻り値はR側ではiterableではないのでちょっと工夫がいる（もっとよい感じのやり方がある気がするが）。

```{r}
mode <- tokenizer$Tokenizer$SplitMode$A
m <- tokenizer_obj$tokenize("国家公務員", mode)
purrr::map(1:py_len(m), ~ list(
  surface = m[.-1]$surface(),
  dic_form = m[.-1]$dictionary_form(),
  reading = m[.-1]$reading_form(),
  part_of_speech = m[.-1]$part_of_speech()
))
```

```{r}
mode <- tokenizer$Tokenizer$SplitMode$B
m <- tokenizer_obj$tokenize("国家公務員", mode)
purrr::map(1:py_len(m), ~ list(
  surface = m[.-1]$surface(),
  dic_form = m[.-1]$dictionary_form(),
  reading = m[.-1]$reading_form(),
  part_of_speech = m[.-1]$part_of_speech()
))
```

```{r}
mode <- tokenizer$Tokenizer$SplitMode$C
m <- tokenizer_obj$tokenize("国家公務員", mode)
purrr::map(1:py_len(m), ~ list(
  surface = m[.-1]$surface(),
  dic_form = m[.-1]$dictionary_form(),
  reading = m[.-1]$reading_form(),
  part_of_speech = m[.-1]$part_of_speech()
))
```

## Normalization

こういうこともできる。

```{r}
mode <- tokenizer$Tokenizer$SplitMode$A
m <- tokenizer_obj$tokenize("まさにッSUMMER★VACATION!!!", mode)
purrr::map(1:py_len(m), ~ list(
  normalized = m[.-1]$normalized_form()
))
```

## セッション情報

```{r}
sessioninfo::session_info()
```
