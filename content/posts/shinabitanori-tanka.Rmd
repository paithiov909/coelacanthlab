---
title: 短歌のテキストマイニング
author: paithiov909
date: "`r Sys.Date()`"
lastmod: '2020-07-11'
slug: shinabitanori-tanka
categories: []
tags:
  - NLP
description: 'Rで短歌のテキストマイニング'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)
stopifnot(require(tidyverse))
```

## ファイルの読み込み

自作短歌のテキストを読み込んで前処理しておきます。

```{r}
normalize <- function(str) {
  str %>%
    stringr::str_replace_all("\u2019", "\'") %>%
    stringr::str_replace_all("\u201d", "\"") %>%
    stringr::str_replace_all("[\u02d7\u058a\u2010\u2011\u2012\u2013\u2043\u207b\u208b\u2212]", "-") %>%
    stringr::str_replace_all("[\ufe63\uff0d\uff70\u2014\u2015\u2500\u2501\u30fc]", enc2utf8("\u30fc")) %>%
    stringr::str_replace_all("[~\u223c\u223e\u301c\u3030\uff5e]", "~") %>%
    stringr::str_remove_all("[:punct:]") %>%
    stringr::str_remove_all("[:blank:]") %>%
    stringr::str_remove_all("[:cntrl:]") %>%
    return()
}

csv <- readr::read_csv("https://bit.ly/2ye2pT6") %>%
  dplyr::mutate(normalized = normalize(body)) %>%
  dplyr::select(id, normalized)

nrow(csv)
```

## 形態素解析

形態素解析の手段はなんでもよいのですが、今回は[この自作パッケージ](https://github.com/paithiov909/tangela)を使います。このパッケージは[atilika/kuromoji](https://github.com/atilika/kuromoji)をrJava経由で呼べるようにしたもので、形態素解析をおこなう関数`tangela::kuromoji`のみを提供するシンプルなものです。

文字列はUTF-8として渡す前提で、戻り値のテキストのエンコーディングもUTF-8になります。

```{r cache=TRUE, cache.path='.cache/'}
parseKuromoji <- function(str) {
  res <- tangela::kuromoji(str)
  res %>%
    purrr::imap_dfr(~
    data.frame(
      mid = .y,
      Surface = .x$surface,
      feature = .x$feature,
      is_unk = .x$is_unk,
      stringsAsFactors = FALSE
    ) %>%
      tidyr::separate(feature, into = c(
        "POS1",
        "POS2",
        "POS3",
        "POS4",
        "X5StageUse1",
        "X5StageUse2",
        "Original",
        "Yomi1",
        "Yomi2"
      ), sep = ",", fill = "right"))
}

res <- csv$normalized %>%
  imap_dfr(function(str, idx) {
    cols <- parseKuromoji(str)
    dplyr::bind_cols(
      data.frame(id = rep(idx, nrow(cols))),
      cols
    )
  }) %>%
  dplyr::mutate(mora = dplyr::if_else(
    .data$is_unk == TRUE,
    nchar(.data$Surface),
    nchar(stringr::str_remove_all(.data$Yomi2, "[\u30e3\u30e5\u30e7]"))
  )) %>%
  tibble::rowid_to_column()

head(res)
```

## 語彙の集計（１）

簡単に語彙の集計をしてみます。{RMeCab}を使わずに手軽に単語文書行列（Document Term Matrix）を作成する方法はいくつかありますが、ここではまずはじめに{textmineR}を使ってみます。文書を文字列ベクトルとして`textmineR::CreateDtm`に渡すだけです。

`textmineR::CreateDtm`に文書を渡すときにストップワードを指定できるので、指定してみています。日本語のストップワードはSlothlibなどが有名ですが、Rパッケージから利用できる面白いものとして、{rtweet}のデータセットである`stopwordslangs`があります。lang, word, pの３カラムからなるデータセットで、langとpでwordを絞り込んで使います。pは0~1の値で、1に近いほどよく使われる単語（英語だったら冠詞のaやtheみたいなやつ）になるみたいな感じです。狭い値の範囲にかなりの数の語彙があるので、p >= 0.98くらいで絞り込んでもふつうに使える気がします。


```{r}
stopwords <- rtweet::stopwordslangs %>%
  dplyr::filter(lang == "ja") %>%
  dplyr::filter(p >= .98) %>%
  dplyr::pull(word)

dtm <- textmineR::CreateDtm( ## この関数はそれなりに重いです
  res$Original,
  doc_names = res$id,
  ngram_window = c(1, 1),
  stopword_vec = stopwords
)
```

プロットしてみます。

```{r}
dtm %>%
  textmineR::TermDocFreq() %>%
  dplyr::arrange(desc(term_freq)) %>%
  head() %>%
  ggplot(aes(x = reorder(term, -doc_freq), y = doc_freq)) +
  geom_col() +
  labs(x = "word") +
  theme_light()
```

## 語彙の集計（２）

単語文書行列を作ったり、文書を便利に扱う別の手段として{quanteda}を使ってみます。{quanteda}パッケージ内では、単語文書行列を含めて、文書に紐づく特徴量を格納した行列一般のことをDocument Feature Matrix（DFM）と呼んでいます。ここではDFMを作ってワードクラウドを描くまでやってみます。

{quanteda}では文書はコーパスというオブジェクトとして持ちます。ふつうに文字列ベクトルやデータフレームを渡して作成できます。

```{r}
corp <- res %>%
  dplyr::group_by(id) %>%
  dplyr::mutate(morphs = stringr::str_c(Surface, collapse = " ")) %>%
  dplyr::ungroup() %>%
  dplyr::select(id, morphs) %>%
  dplyr::distinct() %>%
  dplyr::right_join(csv, by = "id") %>%
  quanteda::corpus(docid_field = "id", text_field = "morphs")

quanteda::ndoc(corp)
```

{quanteda}はコーパスに収めた各文書内での位置情報を保持しながら文書をトークン化して扱うことができます。トークン化は日本語を含めて{stringi}と内臓している辞書を用いてイイ感じにおこなってくれるので、新聞記事のようなさほど砕けていない文章ならトークナイザを用意しなくてもこのパッケージ単体でトークン化することができます（私は使ったことないですが、{spacyr}と連携してPOS taggingしたりといった使い方も想定しているらしいです）。ただ、今回は先にKuromojiで形態素解析をおこなっているので、単純な半角スペース区切りでトークン化するように`quanteda::tokens(what = "fastestword")`に渡します。

トークンオブジェクトは`quanteda::tokens_select`とそのショートハンドの関数でイイ感じに語彙を制限することができます。トークンオブジェクトを以下のようにパイプして、簡単にワードクラウドが描けます。

```{r}
dic <- res %>%
  dplyr::filter(POS1 == "名詞") %>%
  dplyr::pull(Surface)

toks <- quanteda::tokens(corp, what = "fastestword") %>%
  quanteda::tokens_keep(dic, valuetype = "fixed") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed")

toks %>%
  quanteda::dfm() %>%
  quanteda::textplot_wordcloud(
    min_count = 2,
    random_order = FALSE,
    color = viridisLite::cividis(8)
  )
```

このプロット内にたぶん「玉」という語がありますが、ただの「玉」なんて単語が出てくるような覚えはありません。おそらく分かち書きに失敗している単語でしょう。詳細を見てみるためにコロケーションの一部を確認してみます。

```{r}
toks %>%
  quanteda::textstat_collocations(size = 2, min_count = 2) %>%
  as.data.frame(stringsAsFactors = FALSE)
```

「悠木碧」とか「しゃぼん玉」とかが2語に分割されていることが確認できます。{quanteda}でこうした複数に分かれてしまったトークンをひとまとめに扱うには、次のようにします。

```{r}
multiwords <- c("悠 木 碧", "しゃぼん 玉", "山手 線", "洗濯 機", "サディ スティック")
toks %>%
  quanteda::tokens_compound(pattern = quanteda::phrase(multiwords)) %>%
  quanteda::kwic(c("悠_木_碧", "しゃぼん_玉"))
```

もう一度ワードクラウドを描いてみました。

```{r}
toks %>%
  quanteda::tokens_compound(pattern = quanteda::phrase(multiwords)) %>%
  quanteda::dfm() %>%
  quanteda::textplot_wordcloud(
    min_count = 2,
    random_order = FALSE,
    color = viridisLite::cividis(8)
  )
```

## 「AのB」の出現位置

ここから本題です。名詞で助詞の「の」が挟まれている「AのB」のようなかたちの表現を探索して、こういった表現が短歌のどの部分に出現するかを概観したいと思います。

まず、「AのB」のようなかたちの表現を集めてみます。短歌は57577の定型詩で、音数（モーラ数）に注目すると便利なことがあるので、名詞はモーラ数に置き換えながら、このかたちの表現をマイニングします。

```{r}
a_b <- res %>%
  dplyr::filter(Surface == "の") %>%
  dplyr::pull(rowid) %>%
  purrr::keep(~ res$POS1[. - 1] == "名詞" & res$POS1[. + 1] == "名詞") %>%
  purrr::map_chr(~ stringr::str_c(
    res$mora[. - 1],
    res$Surface[.],
    res$mora[. + 1],
    collapse = ""
  )) %>%
  purrr::set_names(
    res %>%
      dplyr::filter(Surface == "の") %>%
      dplyr::pull(rowid) %>%
      purrr::keep(~ res$POS1[. - 1] == "名詞" & res$POS1[. + 1] == "名詞") %>%
      purrr::map_chr(~ stringr::str_c(
        res$Surface[. - 1],
        res$Surface[.],
        res$Surface[. + 1],
        collapse = ""
      ))
  )

head(a_b)
```

同じモーラ数の表現ごとにカウントして可視化してみます。

```{r}
res %>%
  dplyr::filter(Surface == "の") %>%
  dplyr::pull(rowid) %>%
  purrr::keep(~ res$POS1[. - 1] == "名詞" & res$POS1[. + 1] == "名詞") %>%
  purrr::map_chr(~ stringr::str_c(
    res$mora[. - 1],
    res$Surface[.],
    res$mora[. + 1],
    collapse = ""
  )) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  dplyr::group_by_all() %>%
  dplyr::count() %>%
  dplyr::arrange(desc(n)) %>%
  head(15) %>%
  ggplot(aes(x = reorder(., -n), y = n)) +
  geom_col() +
  labs(x = "AのB") +
  theme_light()
```

「AのB」に相当する表現だけ「NのN」（Nはモーラ数）というかたちになるように分かち書きされた文字列ベクトルを作って、{quanteda}のコーパスに格納します。

```{r}
`%without%` <- purrr::negate(`%in%`)

idx <- res %>%
  dplyr::filter(Surface == "の") %>%
  dplyr::pull(rowid) %>%
  purrr::keep(~ res$POS1[. - 1] == "名詞" & res$POS1[. + 1] == "名詞")

corp <- idx %>%
  purrr::map_dfr(~
  data.frame(
    rowid = .,
    compound = stringr::str_c(
      res$mora[. - 1],
      res$Surface[.],
      res$mora[. + 1],
      collapse = ""
    ),
    stringsAsFactors = FALSE
  )) %>%
  dplyr::right_join(
    res %>%
      dplyr::filter(rowid %without% c(idx - 1, idx + 1)) %>%
      dplyr::mutate(compound = Surface),
    by = "rowid"
  ) %>%
  dplyr::group_by(id) %>%
  dplyr::mutate(morphs = stringr::str_c(
    dplyr::if_else(
      is.na(compound.x),
      compound.y,
      compound.x
    ),
    collapse = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(id, morphs) %>%
  dplyr::distinct() %>%
  dplyr::right_join(csv, by = "id") %>%
  quanteda::corpus(docid_field = "id", text_field = "morphs")

quanteda::ndoc(corp)
```

少しだけKWICで確認してみます。

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::kwic(pattern = "[1-9]+の[0-9]+", valuetype = "regex") %>%
  head(30) %>%
  knitr::kable()
```

### 3の2

カウントが多かったものから順に、3の2から4の4まで、短歌のなかでの実際の位置を確認してみましょう。以下にあげる図の黒い線が引かれているあたりがその表現の各短歌内における出現箇所です。下図を見ると、3の2は三句と四句あたりが多そうだけど、わりとまんべんなくあるっぽいです。二句三句をまたぐようなあたりにはなさそう。

```{r}
toks <- corp %>%
  quanteda::tokens(what = "fastestword")
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "3の2", valuetype = "regex")
)
```

```{r}
a_b %>%
  purrr::keep(. == "3の2")
```

### 2の3

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "2の3", valuetype = "regex")
)
```

```{r}
a_b %>%
  purrr::keep(. == "2の3")
```

### 2の2

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "2の2", valuetype = "regex")
)
```

```{r}
a_b %>%
  purrr::keep(. == "2の2")
```

### 4の3

明らかに偏りがあります。初句に多い。

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "4の3", valuetype = "fixed")
)
```

```{r}
a_b %>%
  purrr::keep(. == "4の3")
```

### 4の2

やはり4モーラはじまりのためか初句が多いようす。

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "4の2", valuetype = "fixed")
)
```

```{r}
a_b %>%
  purrr::keep(. == "4の2")
```

### 2の4

三句あたりが多い気がする。

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "2の4", valuetype = "fixed")
)
```

```{r}
a_b %>%
  purrr::keep(. == "2の4")
```

### 3の3

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "3の3", valuetype = "fixed")
)
```

```{r}
a_b %>%
  purrr::keep(. == "3の3")
```

### 4の4

```{r}
quanteda::textplot_xray(
  quanteda::kwic(toks, pattern = "4の4", valuetype = "fixed")
)
```

```{r}
a_b %>%
  purrr::keep(. == "4の4")
```

## セッション情報

```{r}
devtools::session_info()
```
