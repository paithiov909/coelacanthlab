---
title: Rによるテキスト解析
author: paithiov909
date: "`r Sys.Date()`"
lastmod: '2020-07-11'
slug: shinabitanori-google-docs
categories: []
tags:
  - NLP
description: 'Rでテキストマイニング'
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)

stopifnot(require(tidyverse))
googledrive::drive_auth(cache = ".secrets")
```

## Googleドキュメントの読み込み

これまで自分が書いてきた文章について、テキスト解析をしてみます。ここで分析している文章はQrunchにあるブログで読むことができます。

> [あきる野市立希望的観測所](https://shinabitanori.qrunch.io/)

これまでに書いた文章はいくつかの場所にバックアップを取っていて、Googleドキュメントにもバックアップがあります。今回はそれらを{googledrive}で取得し、{readtext}で読み込みます。

`googledrive::drive_download`はディレクトリごとダウンロードしたりはできないようなので、特定のディレクトリにあるファイルのリスト（dribble）を`dplyr::rowwise`で行ごとに渡して`dplyr::do`のなかでダウンロードしています（`dplyr::rowwise() %>% dplyr::do()`という流れで処理するやり方は手元にあるdplyr v0.8.0の時点ですでにquestioningなので、将来的に使えなくなる可能性があります）。


```{r}
aquarium <- googledrive::drive_ls("Documents/aquarium/") %>%
  dplyr::rowwise() %>%
  dplyr::do(googledrive::drive_download(
    googledrive::as_id(.$id),
    path = file.path(tempdir(), .$name),
    overwrite = TRUE,
    verbose = FALSE
  ))
shinabitanori <- googledrive::drive_ls("Documents/shinabitanori/") %>%
  dplyr::rowwise() %>%
  dplyr::do(googledrive::drive_download(
    googledrive::as_id(.$id),
    path = file.path(tempdir(), .$name),
    overwrite = TRUE,
    verbose = FALSE
  ))
```

ダウンロードしたdocxファイルのリストをデータフレームとして持っておきます。文章は公開されている場所などに応じて３つのディレクトリに分けて保存されています。ここでは、この保存されているディレクトリを文書の変数として持つようにします。

```{r}
df <- list("aquarium", "shinabitanori") %>%
  purrr::map_dfr(~
  dplyr::mutate(rlang::eval_tidy(rlang::parse_expr(.)), doc = .)) %>%
  dplyr::select(doc, name, local_path) %>%
  tibble::rowid_to_column()

df[1, ]
```

## 形態素解析

[この自作パッケージ](https://github.com/paithiov909/tangela)を使っています。結果を{quanteda}のコーパスオブジェクトとして格納して、いろいろ試していきます。

```{r}
normalize <- function(str) {
  str %>%
    stringr::str_replace_all("\u2019", "\'") %>%
    stringr::str_replace_all("\u201d", "\"") %>%
    stringr::str_replace_all("[\u02d7\u058a\u2010\u2011\u2012\u2013\u2043\u207b\u208b\u2212]", "-") %>%
    stringr::str_replace_all("[\ufe63\uff0d\uff70\u2014\u2015\u2500\u2501\u30fc]", enc2utf8("\u30fc")) %>%
    stringr::str_replace_all("[~\u223c\u223e\u301c\u3030\uff5e]", "~") %>%
    stringr::str_remove_all("[:punct:]") %>%
    stringr::str_remove_all("[:blank:]") %>%
    stringr::str_remove_all("[:cntrl:]") %>%
    return()
}

corp <- df %>%
  dplyr::rowwise() %>%
  dplyr::do(readtext::readtext(.$local_path, docvarsfrom = "filenames", docvarnames = c("name"))) %>%
  dplyr::bind_rows() %>%
  dplyr::right_join(
    dplyr::select(df, rowid, doc, name),
    by = "name"
  ) %>%
  dplyr::mutate(text = normalize(text)) %>%
  dplyr::mutate(tokens = stringr::str_c(
    purrr::map_chr(
      tangela::kuromoji(text), ~
      purrr::pluck(., "surface")
    ),
    collapse = " "
  )) %>%
  quanteda::corpus(text_field = "tokens")
```

## ワードクラウド

ストップワードとして`rtweet::stopwordslangs`を利用しています。

```{r}
stopwords <- rtweet::stopwordslangs %>%
  dplyr::filter(lang == "ja") %>%
  dplyr::filter(p >= .98) %>%
  dplyr::pull(word)

corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::dfm_trim(min_termfreq = 3L) %>%
  quanteda::textplot_wordcloud(comparison = TRUE, color = viridis::cividis(3))
```

## 出現頻度の集計

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight("prop") %>%
  quanteda::textstat_frequency(groups = "doc") %>%
  dplyr::top_n(-16L, rank) %>%
  ggpubr::ggdotchart(
    x = "feature",
    y = "frequency",
    group = "group",
    color = "group",
    rotate = TRUE
  ) +
  theme_bw()
```

## Keyness

aquariumグループの文書とその他の対照を見ています。

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::textstat_keyness(target = "aquarium") %>%
  quanteda::textplot_keyness()
```

## 対応分析

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda.textmodels::textmodel_ca() %>%
  quanteda.textmodels::textplot_scale1d(
    margin = "documents",
    groups = quanteda::docvars(corp, "doc")
  )
```

## 共起ネットワーク

```{r}
corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::dfm_trim(min_termfreq = 20L) %>%
  quanteda::fcm() %>%
  quanteda::textplot_network(min_freq = .8)
```

## クラスタリング

マンハッタン距離、ward法（ward.D2）です。

```{r}
d <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda::textstat_dist(method = "manhattan", diag = TRUE) %>%
  as.dist() %>%
  hclust(method = "ward.D2") %>%
  ggdendro::dendro_data(type = "rectangle") %>%
  purrr::list_modify(
    labels = dplyr::bind_cols(
      .$labels,
      names = quanteda::docvars(corp, "name"),
      doc = quanteda::docvars(corp, "doc")
    )
  )

ggplot(ggdendro::segment(d)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(ggdendro::label(d), mapping = aes(x, y, label = names, colour = doc, hjust = 0), size = 3) +
  coord_flip() +
  scale_y_reverse(expand = c(.2, 0)) +
  ggdendro::theme_dendro()
```

## LDA（Latent Dirichlet Allocation）

```{r}
dtm <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_tfidf()

features <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "name") %>%
  quanteda::ntoken()

m <- dtm %>%
  as("dgCMatrix") %>%
  textmineR::FitLdaModel(k = 3, iterations = 200, burnin = 175)

m$phi %>%
  textmineR::GetTopTerms(15L) %>%
  knitr::kable()
```

{LDAvis}で可視化してみます。ただ、{LDAvis}はもうしばらくメンテナンスされていないパッケージで、ちょっと挙動があやしいところがあります。たとえば、デフォルトロケールがCP932であるWindows環境の場合、`LDAvis::createJSON`で書き出されるラベル（vocab）のエンコーディングがそっちに引きずられてCP932になってしまうため、ブラウザで表示したときにラベルが文字化けします。書き出されたlda.jsonをUTF-8に変換すれば文字化けは解消されるので、とりあえずあとから変換して上書きするとよいです。

```{r}
LDAvis::createJSON(
  phi = m$phi,
  theta = m$theta,
  doc.length = features,
  vocab = stringi::stri_enc_toutf8(dtm @ Dimnames$features),
  term.frequency = quanteda::colSums(dtm)
) %>%
  LDAvis::serVis(open.browser = FALSE, out.dir = file.path(getwd(), "cache"))

readr::read_lines_raw(file.path(getwd(), "cache", "lda.json")) %>%
  iconv(from = "CP932", to = "UTF-8") %>%
  jsonlite::parse_json(simplifyVector = TRUE) %>%
  jsonlite::write_json(file.path(getwd(), "cache", "lda.json"), dataframe = "columns", auto_unbox = TRUE)
```

```{r, include=FALSE}
targets <- list.files(path = file.path(getwd(), "cache"), all.files = TRUE, full.names = TRUE, no.. = TRUE)

unlink(
  list.files(
    path = file.path(getwd(), "../../", "static", "ldavis", "shinabitanori-google-docs"),
    all.files = TRUE,
    full.names = TRUE,
    no.. = TRUE
  )
)

sapply(targets, function(file) {
  file.copy(
    from = file,
    to = file.path(getwd(), "../../", "static", "ldavis", "shinabitanori-google-docs")
  )
})

remove(targets)
```

> [LDAvis](/ldavis/shinabitanori-google-docs)

## GloVe

```{r}
toks <- corp %>%
  quanteda::tokens(what = "fastestword") %>%
  as.list() %>%
  text2vec::itoken()

vocab <- toks %>%
  text2vec::create_vocabulary() %>%
  text2vec::prune_vocabulary(term_count_min = 5L)

vect <- text2vec::vocab_vectorizer(vocab)

tcm <- text2vec::create_tcm(
  it = toks,
  vectorizer = vect,
  skip_grams_window = 5L
)

glove <- text2vec::GlobalVectors$new(
  rank = 50,
  x_max = 15L
)

wv <- glove$fit_transform(
  x = tcm,
  n_iter = 10L
) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  tibble::as_tibble(.name_repair = "minimal", rownames = NA)
```

{Rtsne}で次元を減らして可視化します。

```{r}
getRtsneAsTbl <- function(tbl, dim = 2, perp = 30) {
  tsn <- tbl %>% Rtsne::Rtsne(dim = dim, perplexity = perp)
  tsny <- tsn$Y
  rownames(tsny) <- row.names(tbl)
  tsny <- as.data.frame(tsny, stringsAsFactors = FALSE)
  return(tibble::as_tibble(tsny, .name_repair = "minimal", rownames = NA))
}

vec <- vocab %>%
  dplyr::anti_join(
    y = tibble::tibble(words = stopwords),
    by = c("term" = "words")
  ) %>%
  dplyr::arrange(desc(term_count)) %>%
  head(100) %>%
  dplyr::left_join(tibble::rownames_to_column(wv), by = c("term" = "rowname")) %>%
  tibble::column_to_rownames("term") %>%
  dplyr::select(V1, V2)

dist <- proxy::dist(x = vec, y = vec, method = "Euclidean", diag = TRUE)
clust <- kmeans(x = dist, centers = 5)
vec <- getRtsneAsTbl(vec, perp = 2) %>%
  tibble::rownames_to_column() %>%
  dplyr::mutate(cluster = as.factor(clust$cluster))

vec %>%
  ggplot(aes(x = V1, y = V2, colour = cluster)) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = rowname)) +
  theme_light()
```

## セッション情報

```{r}
devtools::session_info()
```
